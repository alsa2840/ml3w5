{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c871ee",
   "metadata": {},
   "source": [
    "# ML3 Week 5 â€” Photo->Monet (CycleGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8353f8",
   "metadata": {},
   "source": [
    "sections: Data & EDA -> Model & Training -> Results (images + KID) -> Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9383ecc",
   "metadata": {},
   "source": [
    "## 1) Data & EDA (local paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local folders (Windows). Monet is required; photos are optional.\n",
    "MONET_DIR = r\"C:\\Users\\Almog\\Desktop\\monet_jpg\"\n",
    "PHOTO_DIR = r\"C:\\Users\\Almog\\Desktop\\photo_jpg\"\n",
    "IMG_SIZE = 256\n",
    "\n",
    "assert os.path.isdir(MONET_DIR), f\"Missing: {MONET_DIR}\"\n",
    "\n",
    "def list_images(folder):\n",
    "    \"Return sorted list of image paths (jpg/jpeg/png).\"\n",
    "    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\")\n",
    "    paths = []\n",
    "    for e in exts:\n",
    "        paths += glob.glob(os.path.join(folder, e))\n",
    "    return sorted(paths)\n",
    "\n",
    "monet_files = list_images(MONET_DIR)\n",
    "photo_files = list_images(PHOTO_DIR) if os.path.isdir(PHOTO_DIR) else []\n",
    "\n",
    "print({\"monet_count\": len(monet_files), \"photo_count\": len(photo_files)})\n",
    "\n",
    "def show_grid(paths, title, n=6):\n",
    "    \"Show a small grid to sanity-check the dataset.\"\n",
    "    n = min(n, len(paths))\n",
    "    if n == 0:\n",
    "        print(\"No images for:\", title); return\n",
    "    pick = random.sample(paths, n)\n",
    "    rows, cols = 2, (n + 1)//2\n",
    "    plt.figure(figsize=(3*cols, 3*rows))\n",
    "    for i,p in enumerate(pick):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        im = Image.open(p).convert(\"RGB\")\n",
    "        plt.imshow(im); plt.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_grid(monet_files, \"Monet samples\")\n",
    "if len(photo_files) > 0:\n",
    "    show_grid(photo_files, \"Photo samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b8ef3",
   "metadata": {},
   "source": [
    "### Optional: create tiny synthetic photo set (only if no photo_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a375949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "\n",
    "SYNTHETIC_PHOTO_DIR = os.path.join(MONET_DIR, \"_synthetic_photos_tmp\")\n",
    "\n",
    "if len(photo_files) == 0:\n",
    "    # Simple transform: sharpen + contrast + slight blur to mimic a different domain.\n",
    "    os.makedirs(SYNTHETIC_PHOTO_DIR, exist_ok=True)\n",
    "    out = []\n",
    "    for p in monet_files[:min(300, len(monet_files))]:\n",
    "        im = Image.open(p).convert(\"RGB\")\n",
    "        im = ImageEnhance.Sharpness(im).enhance(2.0)\n",
    "        im = ImageEnhance.Contrast(im).enhance(1.5)\n",
    "        im = im.filter(ImageFilter.GaussianBlur(radius=1))\n",
    "        q = os.path.join(SYNTHETIC_PHOTO_DIR, \"synth_\" + os.path.basename(p))\n",
    "        im.save(q, quality=95)\n",
    "        out.append(q)\n",
    "    photo_files = out\n",
    "    print(\"Synthetic photos created:\", len(photo_files))\n",
    "    show_grid(photo_files, \"Synthetic Photo samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4beb8",
   "metadata": {},
   "source": [
    "## 2) Model & Training (compact CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Short run for coursework; increase for quality.\n",
    "BATCH = 4\n",
    "EPOCHS = 3\n",
    "STEPS_PER_EPOCH = 300\n",
    "\n",
    "def load_tf_image(path, size=IMG_SIZE):\n",
    "    \"Read -> decode -> resize -> normalize to [-1,1].\"\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_image(x, channels=3, expand_animations=False)\n",
    "    x = tf.image.resize(x, [size, size], method='area')\n",
    "    x = tf.cast(x, tf.float32)/127.5 - 1.0\n",
    "    return x\n",
    "\n",
    "class ReflectionPad2D(tf.keras.layers.Layer):\n",
    "    \"Reflection padding used in many style-transfer nets.\"\n",
    "    def __init__(self, pad): super().__init__(); self.pad=pad\n",
    "    def call(self, x): p=self.pad; return tf.pad(x, [[0,0],[p,p],[p,p],[0,0]], mode=\"REFLECT\")\n",
    "\n",
    "def conv_blk(x,f,k=3,s=1,norm=True,act=True):\n",
    "    \"Conv -> (instance-like) norm -> ReLU (optional).\"\n",
    "    x = tf.keras.layers.Conv2D(f,k,s,padding='valid' if k==7 else 'same',use_bias=not norm)(x)\n",
    "    if norm:\n",
    "        m,v = tf.nn.moments(x,[1,2],keepdims=True)\n",
    "        x = (x-m)/tf.sqrt(v+1e-5)\n",
    "    if act: x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def res_block(x,f):\n",
    "    \"Two convs plus skip connection (ResNet block).\"\n",
    "    y = ReflectionPad2D(1)(x)\n",
    "    y = conv_blk(y,f,3,1,True,True)\n",
    "    y = ReflectionPad2D(1)(y)\n",
    "    y = conv_blk(y,f,3,1,True,False)\n",
    "    return tf.keras.layers.Add()([x,y])\n",
    "\n",
    "def build_generator(img_size=256,n_res=6):\n",
    "    \"ResNet-style generator (Monet <-> Photo).\"\n",
    "    i = tf.keras.Input((img_size,img_size,3))\n",
    "    x = ReflectionPad2D(3)(i)\n",
    "    x = conv_blk(x,64,7,1)\n",
    "    x = conv_blk(x,128,3,2)\n",
    "    x = conv_blk(x,256,3,2)\n",
    "    for _ in range(n_res): x = res_block(x,256)\n",
    "    x = tf.keras.layers.Conv2DTranspose(128,3,2,padding=\"same\")(x); x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(64,3,2,padding=\"same\")(x);  x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = ReflectionPad2D(3)(x)\n",
    "    o = tf.keras.layers.Conv2D(3,7,padding=\"valid\",activation=\"tanh\")(x)\n",
    "    return tf.keras.Model(i,o, name=\"G\")\n",
    "\n",
    "def build_discriminator(img_size=256):\n",
    "    \"PatchGAN discriminator (classifies local patches).\"\n",
    "    def d(x,f,s): x=tf.keras.layers.Conv2D(f,4,s,padding=\"same\")(x); return tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    i = tf.keras.Input((img_size,img_size,3))\n",
    "    x = d(i,64,2); x=d(x,128,2); x=d(x,256,2); x=d(x,512,1)\n",
    "    o = tf.keras.layers.Conv2D(1,4,padding=\"same\")(x)\n",
    "    return tf.keras.Model(i,o, name=\"D\")\n",
    "\n",
    "# Loss helpers\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "def gan_loss(logits, is_real):  # LSGAN style\n",
    "    y = tf.ones_like(logits) if is_real else tf.zeros_like(logits)\n",
    "    return mse(y, logits)\n",
    "\n",
    "def make_ds(paths):\n",
    "    \"TF pipeline: shuffle -> decode/resize/normalize -> batch -> prefetch.\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds = ds.shuffle(1000, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda p: load_tf_image(p), num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_TRAINING = len(photo_files) > 0\n",
    "print(\"Training enabled:\", RUN_TRAINING)\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    photo_ds = make_ds(photo_files); monet_ds = make_ds(monet_files)\n",
    "    paired = tf.data.Dataset.zip((photo_ds, monet_ds)).repeat()\n",
    "\n",
    "    G = build_generator(); F = build_generator()\n",
    "    Dx = build_discriminator(); Dy = build_discriminator()\n",
    "\n",
    "    # Single Adam instance is fine for this mini-project\n",
    "    g_opt=f_opt=dx_opt=dy_opt=tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    LAMBDA_CYCLE=10.0; LAMBDA_ID=0.5  # cycle & identity weights\n",
    "\n",
    "    @tf.function\n",
    "    def step(rx, ry):\n",
    "        \"One training step for both directions plus both discriminators.\"\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            # Generators: Photo->Monet (G) and Monet->Photo (F)\n",
    "            fy = G(rx, training=True); cx = F(fy, training=True)  # cycle back\n",
    "            fx = F(ry, training=True); cy = G(fx, training=True)  # cycle back\n",
    "            sx = F(rx, training=True); sy = G(ry, training=True)  # identity preserve\n",
    "            # Discriminators (PatchGAN logits)\n",
    "            dxr = Dx(rx, True); dxf = Dx(fx, True)\n",
    "            dyr = Dy(ry, True); dyf = Dy(fy, True)\n",
    "            # Losses\n",
    "            g_adv = gan_loss(dyf, True); f_adv = gan_loss(dxf, True)\n",
    "            cyc = mae(rx, cx) + mae(ry, cy); idt = mae(rx, sx) + mae(ry, sy)\n",
    "            g_tot = g_adv + LAMBDA_CYCLE*cyc + LAMBDA_ID*idt\n",
    "            f_tot = f_adv + LAMBDA_CYCLE*cyc + LAMBDA_ID*idt\n",
    "            dx_loss = 0.5*(gan_loss(dxr, True)+gan_loss(dxf, False))\n",
    "            dy_loss = 0.5*(gan_loss(dyr, True)+gan_loss(dyf, False))\n",
    "\n",
    "        # Apply gradients\n",
    "        g_opt.apply_gradients(zip(t.gradient(g_tot, G.trainable_variables), G.trainable_variables))\n",
    "        f_opt.apply_gradients(zip(t.gradient(f_tot, F.trainable_variables), F.trainable_variables))\n",
    "        dx_opt.apply_gradients(zip(t.gradient(dx_loss, Dx.trainable_variables), Dx.trainable_variables))\n",
    "        dy_opt.apply_gradients(zip(t.gradient(dy_loss, Dy.trainable_variables), Dy.trainable_variables))\n",
    "        return g_tot, f_tot, dx_loss, dy_loss\n",
    "\n",
    "    import os\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        it = iter(paired)\n",
    "        for s in range(STEPS_PER_EPOCH):\n",
    "            gL,fL,dxL,dyL = step(*next(it))\n",
    "            if (s+1)%100==0:\n",
    "                print(f\"E{e+1}/{EPOCHS} S{s+1}: G {gL.numpy():.3f} F {fL.numpy():.3f} Dx {dxL.numpy():.3f} Dy {dyL.numpy():.3f}\")\n",
    "    G.save(\"weights/photo2monet_generator.keras\")\n",
    "    print(\"Saved weights to weights/photo2monet_generator.keras\")\n",
    "else:\n",
    "    print(\"Training skipped (no photos).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c942a0a",
   "metadata": {},
   "source": [
    "## 3) Results & Metric (KID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "def to_uint8(x):\n",
    "    \"[-1,1] -> [0,255] uint8 for display.\"\n",
    "    x = (x*0.5 + 0.5)\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "    return (x*255).astype('uint8')\n",
    "\n",
    "def load_generator(path=\"weights/photo2monet_generator.keras\"):\n",
    "    \"Load trained generator if exists.\"\n",
    "    if os.path.exists(path):\n",
    "        return tf.keras.models.load_model(path, compile=False)\n",
    "    return None\n",
    "\n",
    "G_loaded = load_generator()\n",
    "\n",
    "# Visual check: input vs Monetized\n",
    "if G_loaded and len(photo_files) > 0:\n",
    "    picks = photo_files[:6]\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i,p in enumerate(picks):\n",
    "        x = load_tf_image(p)[None,...]\n",
    "        y = G_loaded(x, training=False)[0].numpy()\n",
    "        plt.subplot(2, len(picks), i+1); plt.imshow(Image.open(p)); plt.axis(\"off\"); plt.title(\"Input\")\n",
    "        plt.subplot(2, len(picks), len(picks)+i+1); plt.imshow(to_uint8(y)); plt.axis(\"off\"); plt.title(\"Monetized\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # KID: MMD on Inception features (lower is better)\n",
    "    inc = InceptionV3(include_top=False, pooling='avg', weights='imagenet')\n",
    "\n",
    "    def inception_features(images_uint8):\n",
    "        images = tf.convert_to_tensor(images_uint8, dtype=tf.float32)\n",
    "        images = tf.image.resize(images, [299,299], method='bilinear')\n",
    "        images = preprocess_input(images)\n",
    "        return inc(images, training=False).numpy()\n",
    "\n",
    "    def polynomial_kernel(c=1.0, degree=3, gamma=None):\n",
    "        def k(a,b):\n",
    "            g = (1.0 / a.shape[1]) if gamma is None else gamma\n",
    "            return (g * a @ b.T + c) ** degree\n",
    "        return k\n",
    "\n",
    "    def mmd_unbiased(polyk, X, Y):\n",
    "        n = X.shape[0]; m = Y.shape[0]\n",
    "        kxx = (polyk(X,X) - np.eye(n)).sum() / (n*(n-1))\n",
    "        kyy = (polyk(Y,Y) - np.eye(m)).sum() / (m*(m-1))\n",
    "        kxy = polyk(X,Y).mean()\n",
    "        return kxx + kyy - 2*kxy\n",
    "\n",
    "    # Collect real Monet and generated samples\n",
    "    real_uint8, gen_uint8 = [], []\n",
    "    for p in random.sample(monet_files, min(64, len(monet_files))):\n",
    "        im = load_tf_image(p).numpy(); im = (im*0.5 + 0.5); real_uint8.append((im*255).astype('uint8'))\n",
    "    for p in random.sample(photo_files, min(64, len(photo_files))):\n",
    "        x = load_tf_image(p)[None,...]; y = G_loaded(x, training=False)[0].numpy()\n",
    "        y = (y*0.5 + 0.5); gen_uint8.append((y*255).astype('uint8'))\n",
    "    real_uint8 = np.stack(real_uint8, axis=0); gen_uint8 = np.stack(gen_uint8, axis=0)\n",
    "\n",
    "    # Compute feature embeddings and KID\n",
    "    f_real = inception_features(real_uint8)\n",
    "    f_gen  = inception_features(gen_uint8)\n",
    "    K = polynomial_kernel(c=1.0, degree=3, gamma=1.0/f_real.shape[1])\n",
    "    kid = mmd_unbiased(K, f_real, f_gen)\n",
    "    print({\"KID_estimate\": float(kid)})\n",
    "else:\n",
    "    print(\"No generator or no photos; only EDA available this run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5300ed6",
   "metadata": {},
   "source": [
    "## 4) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee16c82",
   "metadata": {},
   "source": [
    "We validated Monet data, trained a compact CycleGAN (when photos exist or when synthetic fallback is used), and reported KID as a simple proxy metric. For better quality, increase training and use a real photo set."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
